{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare parallel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import nltk.data\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS_CSV = 'src/links.csv'\n",
    "FR_TSV = 'src/fra_sentences.tsv'\n",
    "EN_TSV = 'src/eng_sentences.tsv'\n",
    "EN_FR_TSV = 'src/Tatoeba_English-French-2021-03-24.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT1 - Read EN, FR and links separately\n",
    "fr = pd.read_csv(FR_TSV, sep='\\t', header=None, error_bad_lines=False)\n",
    "fr = fr.rename(columns={0:'ID',1:'LANG',2:'TEXT'})\n",
    "\n",
    "en = pd.read_csv(EN_TSV, sep='\\t', header=None, error_bad_lines=False)\n",
    "en = en.rename(columns={0:'ID',1:'LANG',2:'TEXT'})\n",
    "\n",
    "links = pd.read_csv(LINKS_CSV, sep='\\t', header=None, error_bad_lines=False)\n",
    "links = links.rename(columns={0:'ID1',1:'ID2'})\n",
    "\n",
    "#Merge to one parallel list\n",
    "one = en[en['ID'].isin(links['ID1'])].copy()\n",
    "two = fr[fr['ID'].isin(links['ID2'])].copy()\n",
    "\n",
    "enfr = one.merge(two.merge(links,left_on='ID',right_on='ID2'),left_on='ID',right_on='ID1',suffixes=('1', '2'))\n",
    "enfr = enfr.loc[:,~enfr.columns.duplicated()]\n",
    "enfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT2 - Read parallel file\n",
    "enfr = pd.read_csv(EN_FR_TSV, sep='\\t', header=None, error_bad_lines=False)\n",
    "enfr = enfr.rename(columns={0:'ID1',1:'TEXT1',2:'ID2',3:'TEXT2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "tokenizer_en = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "def tokenize_en(string, to_lower = False):\n",
    "    #apostrophe fix\n",
    "    string = string.replace(\"`\", \"'\")\n",
    "    string = string.replace(\"’\", \"'\")\n",
    "    \n",
    "    if to_lower:\n",
    "        string = string.lower()\n",
    "    string = string.strip()\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for sent in tokenizer_en.tokenize(string):\n",
    "        sent_tokens = [tok.replace(\"'\",\"\") for tok in toktok.tokenize(sent) if any(c.isalpha() for c in tok)]\n",
    "        tokens.extend(sent_tokens)\n",
    "    \n",
    "    #sent_tokens = [tok.replace(\"'\",\"\") for tok in word_tokenize(string, language='english') if any(c.isalpha() for c in tok)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_fr(string, to_lower=False):\n",
    "    #apostrophe fix\n",
    "    string = string.replace(\"`\", \"'\")\n",
    "    string = string.replace(\"’\", \"'\")\n",
    "    \n",
    "    if to_lower:\n",
    "        string = string.lower()\n",
    "    string = string.strip()\n",
    "    \n",
    "    tokens = word_tokenize(string, language='french')\n",
    "    \n",
    "    real_tokens = []\n",
    "    for t in tokens:\n",
    "        if any(c.isalpha() for c in t):\n",
    "            subtokens = t.split(\"'\")\n",
    "            if len(subtokens) > 1:\n",
    "                subtokens = [s + \"'\" for s in subtokens[:-1]] + [subtokens[-1]]\n",
    "            real_tokens.extend(subtokens)\n",
    "\n",
    "    \n",
    "    return real_tokens\n",
    "    \n",
    "# def tokenize(string, to_lower = False, clear_nonalpha = False):\n",
    "#     #apostrophe fix\n",
    "#     string = string.replace(\"`\", \"'\")\n",
    "#     string = string.replace(\"’\", \"'\")\n",
    "    \n",
    "#     tokens = []\n",
    "#     if to_lower:\n",
    "#         string = string.lower()\n",
    "#     string = string.strip()\n",
    "\n",
    "#     for sent in tokenizer_en.tokenize(string):\n",
    "#         if clear_nonalpha:\n",
    "#             sent_tokens = [tok for tok in toktok.tokenize(sent) if tok.isalpha()]\n",
    "#         else:\n",
    "#             sent_tokens = [tok for tok in toktok.tokenize(sent)]\n",
    "        \n",
    "#         tokens.extend(sent_tokens)\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295583 parallel sents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID1</th>\n",
       "      <th>TEXT1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>TEXT2</th>\n",
       "      <th>WORDS1</th>\n",
       "      <th>WORDS2</th>\n",
       "      <th>NOWORDS1</th>\n",
       "      <th>NOWORDS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1276</td>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>3091</td>\n",
       "      <td>Essayons quelque chose !</td>\n",
       "      <td>[let, s, try, something]</td>\n",
       "      <td>[essayons, quelque, chose]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1276</td>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>456963</td>\n",
       "      <td>Tentons quelque chose !</td>\n",
       "      <td>[let, s, try, something]</td>\n",
       "      <td>[tentons, quelque, chose]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>373908</td>\n",
       "      <td>Je dois aller dormir.</td>\n",
       "      <td>[i, have, to, go, to, sleep]</td>\n",
       "      <td>[je, dois, aller, dormir]</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1280</td>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>3095</td>\n",
       "      <td>Aujourd'hui nous sommes le 18 juin et c'est l'...</td>\n",
       "      <td>[today, is, june, 18th, and, it, is, muiriel, ...</td>\n",
       "      <td>[aujourd', hui, nous, sommes, le, juin, et, c'...</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280</td>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>696081</td>\n",
       "      <td>Aujourd'hui c'est le 18 juin, et c'est l'anniv...</td>\n",
       "      <td>[today, is, june, 18th, and, it, is, muiriel, ...</td>\n",
       "      <td>[aujourd', hui, c', est, le, juin, et, c', est...</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID1                                             TEXT1     ID2  \\\n",
       "0  1276                              Let's try something.    3091   \n",
       "1  1276                              Let's try something.  456963   \n",
       "2  1277                            I have to go to sleep.  373908   \n",
       "3  1280  Today is June 18th and it is Muiriel's birthday!    3095   \n",
       "4  1280  Today is June 18th and it is Muiriel's birthday!  696081   \n",
       "\n",
       "                                               TEXT2  \\\n",
       "0                           Essayons quelque chose !   \n",
       "1                            Tentons quelque chose !   \n",
       "2                              Je dois aller dormir.   \n",
       "3  Aujourd'hui nous sommes le 18 juin et c'est l'...   \n",
       "4  Aujourd'hui c'est le 18 juin, et c'est l'anniv...   \n",
       "\n",
       "                                              WORDS1  \\\n",
       "0                           [let, s, try, something]   \n",
       "1                           [let, s, try, something]   \n",
       "2                       [i, have, to, go, to, sleep]   \n",
       "3  [today, is, june, 18th, and, it, is, muiriel, ...   \n",
       "4  [today, is, june, 18th, and, it, is, muiriel, ...   \n",
       "\n",
       "                                              WORDS2  NOWORDS1  NOWORDS2  \n",
       "0                         [essayons, quelque, chose]         4         3  \n",
       "1                          [tentons, quelque, chose]         4         3  \n",
       "2                          [je, dois, aller, dormir]         6         4  \n",
       "3  [aujourd', hui, nous, sommes, le, juin, et, c'...        10        13  \n",
       "4  [aujourd', hui, c', est, le, juin, et, c', est...        10        13  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enfr['WORDS1'] = enfr['TEXT1'].apply(lambda x: tokenize_en(x, True))\n",
    "enfr['WORDS2'] = enfr['TEXT2'].apply(lambda x: tokenize_fr(x, True))\n",
    "\n",
    "enfr['NOWORDS1'] = enfr['WORDS1'].apply(lambda x: len(x))\n",
    "enfr['NOWORDS2'] = enfr['WORDS2'].apply(lambda x: len(x))\n",
    "\n",
    "enfr_raw = enfr.copy() #backup original\n",
    "\n",
    "print(\"%i parallel sents\"%len(enfr))\n",
    "\n",
    "enfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba filtering\n",
    "with open('etc/boynames_EN.txt', 'r') as f:\n",
    "    boynames_en = [name[0:-1] for name in f.readlines()]\n",
    "with open('etc/girlnames_EN.txt', 'r') as f:\n",
    "    girlnames_en = [name[0:-1] for name in f.readlines()]\n",
    "with open('etc/avoidlist_EN.txt', 'r') as f:\n",
    "    avoidlist_en = [word[0:-1] for word in f.readlines()]\n",
    "    \n",
    "with open('etc/boynames_FR.txt', 'r') as f:\n",
    "    boynames_fr = [name[0:-1] for name in f.readlines()]\n",
    "with open('etc/girlnames_FR.txt', 'r') as f:\n",
    "    girlnames_fr = [name[0:-1] for name in f.readlines()]\n",
    "with open('etc/avoidlist_FR.txt', 'r') as f:\n",
    "    avoidlist_fr = [word[0:-1] for word in f.readlines()]\n",
    "\n",
    "MAX_COMMAS = 7\n",
    "MAX_PERIOD = 4\n",
    "MAX_QUOTE_EN = 4\n",
    "MAX_QUOTE_FR = 2\n",
    "MAX_TOKENS = 40\n",
    "AVOID_CHAR = ['/', '\\*', '<','₂','™','√','⊂','□','→','₫','=','>','\\[','\\]','\\^','_','~', '§', '´', '‽']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char cleaning: 295512\n"
     ]
    }
   ],
   "source": [
    "# Filter sentences with bad chars\n",
    "mask1 = enfr[\"TEXT1\"].str.contains(r'(?:{})'.format('|'.join(AVOID_CHAR)))\n",
    "mask2 = enfr[\"TEXT2\"].str.contains(r'(?:{})'.format('|'.join(AVOID_CHAR)))\n",
    "enfr = enfr[~mask1 & ~mask2]\n",
    "print(\"Char cleaning:\", len(enfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After bad word cleaning: 288100\n"
     ]
    }
   ],
   "source": [
    "# # Filter sentences with bad words\n",
    "mask1 = enfr[\"TEXT1\"].str.contains(r'\\b(?:{})\\b'.format('|'.join(avoidlist_en)))\n",
    "mask2 = enfr[\"TEXT2\"].str.contains(r'\\b(?:{})\\b'.format('|'.join(avoidlist_fr)))\n",
    "enfr = enfr[~mask1 & ~mask2]\n",
    "print(\"After bad word cleaning:\", len(enfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After length filters: 287020\n"
     ]
    }
   ],
   "source": [
    "# Filter sentences that are like conversation and too long\n",
    "def filter_length(row):\n",
    "    if len(row['TEXT1'].split()) > MAX_TOKENS or \\\n",
    "       len(row['TEXT2'].split()) > MAX_TOKENS or \\\n",
    "       row['TEXT1'].count(',') > MAX_COMMAS or \\\n",
    "       row['TEXT2'].count(',') > MAX_COMMAS or \\\n",
    "       row['TEXT1'].count('. ') > MAX_PERIOD or \\\n",
    "       row['TEXT2'].count('. ') > MAX_PERIOD or \\\n",
    "       row['TEXT1'].count('\"') > MAX_QUOTE_EN or \\\n",
    "       row['TEXT2'].count('\"') > MAX_QUOTE_EN or \\\n",
    "       row['TEXT2'].count('«') > MAX_QUOTE_FR or \\\n",
    "      (row['TEXT1'].startswith('\"') and row['TEXT1'].endswith('\"')) or \\\n",
    "      (row['TEXT2'].startswith('«') and row['TEXT2'].endswith('»')):\n",
    "#         print(row['TEXT1'])\n",
    "#         print(row['TEXT2'])\n",
    "#         print()\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "mask = enfr.apply(filter_length, axis=1)\n",
    "enfr = enfr[mask]\n",
    "print(\"After length filters:\", len(enfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def diversify_names(row):\n",
    "    if 'mary' in row[\"WORDS1\"]:\n",
    "        agirlname = girlnames_en[random.randint(0,len(girlnames_en) - 1)]\n",
    "        \n",
    "        row[\"TEXT1\"] = row[\"TEXT1\"].replace('Mary', agirlname)\n",
    "        row[\"TEXT2\"] = row[\"TEXT2\"].replace('Mary', agirlname)\n",
    "        \n",
    "    if 'tom' in row[\"WORDS2\"]:\n",
    "        aboyname = boynames_en[random.randint(0,len(boynames_en) - 1)]\n",
    "        row[\"TEXT1\"] = row[\"TEXT1\"].replace('Tom', aboyname)\n",
    "        row[\"TEXT2\"] = row[\"TEXT2\"].replace('Tom', aboyname)\n",
    "\n",
    "    return row\n",
    "\n",
    "enfr = enfr.apply(diversify_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read or write bigset from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "enfr.to_csv('enfr.tsv', sep=\"\\t\")\n",
    "enfr.to_pickle('enfr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from file\n",
    "enfr = pd.read_pickle(\"enfr.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract reference vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPUS_VOCAB_EN = 'src/opus_en_50k.txt'\n",
    "OPUS_VOCAB_FR = 'src/opus_fr_50k.txt'\n",
    "REFERENCE_VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_opus_vocab(vocab_file, size=None):\n",
    "    vocab = pd.read_csv(vocab_file, sep=' ', header=None, error_bad_lines=False)\n",
    "    vocab = vocab.rename(columns={0:'WORD',1:'COUNT'})\n",
    "    \n",
    "    vocab[\"FREQ\"] = vocab[\"COUNT\"] / vocab['COUNT'].sum()\n",
    "    \n",
    "    vocab.reset_index(inplace=True)\n",
    "\n",
    "    if size: \n",
    "        vocab = vocab.head(size)\n",
    "    return vocab\n",
    "\n",
    "opus_vocab_en = read_opus_vocab(OPUS_VOCAB_EN, REFERENCE_VOCAB_SIZE)\n",
    "opus_vocab_fr = read_opus_vocab(OPUS_VOCAB_FR, REFERENCE_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>WORD</th>\n",
       "      <th>COUNT</th>\n",
       "      <th>FREQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>you</td>\n",
       "      <td>22484400</td>\n",
       "      <td>0.042494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i</td>\n",
       "      <td>19975318</td>\n",
       "      <td>0.037752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>17594291</td>\n",
       "      <td>0.033252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>13200962</td>\n",
       "      <td>0.024949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>11230036</td>\n",
       "      <td>0.021224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index WORD     COUNT      FREQ\n",
       "0      0  you  22484400  0.042494\n",
       "1      1    i  19975318  0.037752\n",
       "2      2  the  17594291  0.033252\n",
       "3      3   to  13200962  0.024949\n",
       "4      4    a  11230036  0.021224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(opus_vocab_en))\n",
    "opus_vocab_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(row, vocab, side=\"1\"):\n",
    "    for w in row[\"WORDS\"+side]:\n",
    "        vocab.add(w)\n",
    "        \n",
    "def analyze_vocab(vocab_set, reference_vocab_df, threshold):\n",
    "    covered = reference_vocab_df.apply(lambda x:x[\"WORD\"] in vocab_set, axis=1) \n",
    "    missing = ~covered\n",
    "\n",
    "    no_covered = (covered[0:threshold]).value_counts()[1]\n",
    "    no_missing = (covered[0:threshold]).value_counts()[0]\n",
    "    \n",
    "    print(\"Vocab size:\", len(vocab_set))\n",
    "    print(\"#Covered: %i (%.2f%%)\"%(no_covered, no_covered/threshold * 100))\n",
    "    print(\"#Missing: %i (%.2f%%)\"%(no_missing, no_missing/threshold * 100))\n",
    "    \n",
    "    missing_df = reference_vocab_df[missing][reference_vocab_df[missing][\"index\"] < threshold]\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=Complete set (enfr)=\")\n",
    "print(\"Size\", len(enfr))\n",
    "\n",
    "print(\"\\n.English.\")\n",
    "vocab_bigset_en = set()\n",
    "_ = enfr.apply(make_vocab, args=(vocab_bigset_en,), axis=1)\n",
    "vocab_bigset_missing_en = analyze_vocab(vocab_bigset_en, opus_vocab_en, 5000)\n",
    "\n",
    "print(\"\\n.French.\")\n",
    "vocab_bigset_fr = set()\n",
    "_ = enfr.apply(make_vocab, args=(vocab_bigset_fr,\"2\",), axis=1)\n",
    "vocab_bigset_missing_fr = analyze_vocab(vocab_bigset_fr, opus_vocab_fr, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Diversity scoring of sentences\n",
    "enfr_random = enfr.sample(n=60000)\n",
    "enfr_1k = enfr_random[0:1000].copy()\n",
    "enfr_5k = enfr_random[0:5000].copy()\n",
    "enfr_10k = enfr_random[5000:15000].copy()\n",
    "enfr_15k = enfr_random[15000:30000].copy()\n",
    "enfr_30k = enfr_random[30000:60000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=Random set=\")\n",
    "s = enfr_30k\n",
    "\n",
    "print(\"Size\", len(s))\n",
    "\n",
    "print(\"\\n.English.\")\n",
    "vocab_en = set()\n",
    "_ = s.apply(make_vocab, args=(vocab_en,), axis=1)\n",
    "missing_en = analyze_vocab(vocab_en, opus_vocab_en, 5000)\n",
    "\n",
    "print(\"\\n.French.\")\n",
    "vocab_fr = set()\n",
    "_ = s.apply(make_vocab, args=(vocab_fr,\"2\",), axis=1)\n",
    "missing_fr = analyze_vocab(vocab_fr, opus_vocab_fr, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity sampling\n",
    "Go word by word in vocabularies and sample from the bigset to a working set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "stopWords_en = set(stopwords.words('english'))\n",
    "stopWords_fr = set(stopwords.words('french'))\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size_to_reach = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose universal set to pick from\n",
    "universal = enfr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words already picked\n",
    "picked_set = pd.read_pickle(\"picked.pickle\")\n",
    "bigset = universal.loc[universal.index.difference(picked_set.index)] # remove already picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (If starting from scratch) initialize empty picked set\n",
    "picked_set = pd.DataFrame(columns=bigset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (For new set) Initialize empty working set\n",
    "working_vocab_en = set()\n",
    "working_vocab_fr = set()\n",
    "working_set = pd.DataFrame(columns=bigset.columns)\n",
    "couldnt_find_word_en = []\n",
    "couldnt_find_word_fr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To continue working on a set\n",
    "working_set = pd.read_pickle(\"minikit.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "# Filtering a pre-prepared set\n",
    "#shit_sents = [403859,255737,3008457,663918,418877,321355,1356999,1152252,6479697,915564,1126115,470742,4545114,1533812,6901123,1183759,328604,592350,2441411,2158898,4443276,1392457,434306,1125593,696955,9035320,518410,632055,710980,2765416,1469180,799217,408712,329620,524515,629763,62026,8122907,1130113,1474763,838740,477377,7589503,941997,5789142,708881,1152184,6329336,691424,32058,777062,66859,865548,883876,779219,3874107,8846435,4261029,326773,6386085,281413,1074856,2888686]\n",
    "shit_sents = []\n",
    "working_set = working_set[~working_set[\"ID1\"].isin(shit_sents)]\n",
    "working_set = working_set.drop_duplicates(subset=['ID1']).drop_duplicates(subset=['ID2'])\n",
    "print(len(working_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-in with vocabulary coverage (English)\n",
    "for w in tqdm(opus_vocab_en[\"WORD\"]):\n",
    "    if w not in stopWords_en and w not in working_vocab_en:\n",
    "        #find sentences containing the word\n",
    "        m = bigset[\"WORDS1\"].apply(lambda x:w in x)\n",
    "        \n",
    "        #make sure it's not already in the working_set(check index, and sentence ids)\n",
    "        sample_from = bigset.loc[bigset[m].index.difference(working_set.index)]\n",
    "        duplicated = list(working_set[working_set[\"ID1\"].isin(list(sample_from[\"ID1\"].values))][\"ID1\"].values)\n",
    "        sample_from = sample_from[~sample_from[\"ID1\"].isin(duplicated)]\n",
    "        duplicated = list(working_set[working_set[\"ID2\"].isin(list(sample_from[\"ID2\"].values))][\"ID2\"].values)\n",
    "        sample_from = sample_from[~sample_from[\"ID2\"].isin(duplicated)]\n",
    "        \n",
    "        if len(sample_from):\n",
    "            #take random sentence\n",
    "            some_sent = sample_from.sample(1)  #sampling size could be larger for more frequent words\n",
    "            #append it to working_set\n",
    "            working_set = working_set.append(some_sent)\n",
    "            #update working vocabularies \n",
    "            working_vocab_en.update(some_sent[\"WORDS1\"].item())\n",
    "            working_vocab_fr.update(some_sent[\"WORDS2\"].item())\n",
    "            \n",
    "            #TODO: To speed up, remove sent from bigset\n",
    "            \n",
    "            #stop when objective set size is reached\n",
    "            if len(working_set) == set_size_to_reach:\n",
    "                print(\"Set size reached before going over all vocabulary\")\n",
    "                break\n",
    "        else:\n",
    "            couldnt_find_word_en.append(w)\n",
    "            \n",
    "print(\"Current size:\", len(working_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-in with vocabulary coverage (French)\n",
    "for w in tqdm(opus_vocab_fr[\"WORD\"]):\n",
    "    if w not in stopWords_fr and w not in working_vocab_fr:\n",
    "        #find sentences containing the word\n",
    "        m = bigset[\"WORDS2\"].apply(lambda x:w in x)\n",
    "        \n",
    "        #make sure it's not already in the working_set (check index, and sentence ids)\n",
    "        sample_from = bigset.loc[bigset[m].index.difference(working_set.index)]\n",
    "        duplicated = list(working_set[working_set[\"ID1\"].isin(list(sample_from[\"ID1\"].values))][\"ID1\"].values)\n",
    "        sample_from = sample_from[~sample_from[\"ID1\"].isin(duplicated)]\n",
    "        duplicated = list(working_set[working_set[\"ID2\"].isin(list(sample_from[\"ID2\"].values))][\"ID2\"].values)\n",
    "        sample_from = sample_from[~sample_from[\"ID2\"].isin(duplicated)]\n",
    "        \n",
    "        if len(sample_from):\n",
    "            #take random sentence\n",
    "            some_sent = sample_from.sample(1)  #sampling size could be larger for more frequent words\n",
    "            #append it to working_set\n",
    "            working_set = working_set.append(some_sent)\n",
    "            #update working vocabularies \n",
    "            working_vocab_en.update(some_sent[\"WORDS1\"].item())\n",
    "            working_vocab_fr.update(some_sent[\"WORDS2\"].item())\n",
    "            \n",
    "            #stop when objective set size is reached\n",
    "            if len(working_set) == set_size_to_reach:\n",
    "                print(\"Set size reached before going over all vocabulary\")\n",
    "                break\n",
    "        else:\n",
    "            couldnt_find_word_fr.append(w)\n",
    "            \n",
    "print(\"Current size:\", len(working_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: 4998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID1</th>\n",
       "      <th>TEXT1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>TEXT2</th>\n",
       "      <th>WORDS1</th>\n",
       "      <th>WORDS2</th>\n",
       "      <th>NOWORDS1</th>\n",
       "      <th>NOWORDS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265717</th>\n",
       "      <td>51702</td>\n",
       "      <td>Every boy is supposed to wear a tie at the party.</td>\n",
       "      <td>8444110</td>\n",
       "      <td>Tous les garçons sont censés porter une cravat...</td>\n",
       "      <td>[every, boy, is, supposed, to, wear, a, tie, a...</td>\n",
       "      <td>[tous, les, garçons, sont, censés, porter, une...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58787</th>\n",
       "      <td>318868</td>\n",
       "      <td>As our father got ill, we had to live on a sma...</td>\n",
       "      <td>134826</td>\n",
       "      <td>Comme notre père est tombé malade, nous allons...</td>\n",
       "      <td>[as, our, father, got, ill, we, had, to, live,...</td>\n",
       "      <td>[comme, notre, père, est, tombé, malade, nous,...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID1                                              TEXT1      ID2  \\\n",
       "265717   51702  Every boy is supposed to wear a tie at the party.  8444110   \n",
       "58787   318868  As our father got ill, we had to live on a sma...   134826   \n",
       "\n",
       "                                                    TEXT2  \\\n",
       "265717  Tous les garçons sont censés porter une cravat...   \n",
       "58787   Comme notre père est tombé malade, nous allons...   \n",
       "\n",
       "                                                   WORDS1  \\\n",
       "265717  [every, boy, is, supposed, to, wear, a, tie, a...   \n",
       "58787   [as, our, father, got, ill, we, had, to, live,...   \n",
       "\n",
       "                                                   WORDS2  NOWORDS1  NOWORDS2  \n",
       "265717  [tous, les, garçons, sont, censés, porter, une...        11        11  \n",
       "58787   [comme, notre, père, est, tombé, malade, nous,...        13        14  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fill in random (long) sentences \n",
    "print(\"Current size:\", len(working_set))\n",
    "\n",
    "fillin_sample = bigset[bigset[\"NOWORDS1\"] > 10]\n",
    "\n",
    "fillin_sample = fillin_sample.loc[fillin_sample.index.difference(working_set.index)]\n",
    "duplicated = list(working_set[working_set[\"ID1\"].isin(list(fillin_sample[\"ID1\"].values))][\"ID1\"].values)\n",
    "fillin_sample = fillin_sample[~fillin_sample[\"ID1\"].isin(duplicated)]\n",
    "duplicated = list(working_set[working_set[\"ID2\"].isin(list(fillin_sample[\"ID2\"].values))][\"ID2\"].values)\n",
    "fillin_sample = fillin_sample[~fillin_sample[\"ID2\"].isin(duplicated)]\n",
    "\n",
    "fillin_sample = fillin_sample.sample(set_size_to_reach - len(working_set))\n",
    "fillin_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 2\n",
      "Final size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Add fillin sample\n",
    "working_set = pd.concat([working_set, fillin_sample])\n",
    "\n",
    "print(\"added\", len(fillin_sample))\n",
    "print(\"Final size:\", len(working_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=Working set=\n",
      "Size 5000\n",
      "\n",
      ".English.\n",
      "Vocab size: 7427\n",
      "#Covered: 4753 (95.06%)\n",
      "#Missing: 247 (4.94%)\n",
      "Avg. words: 8.6958\n",
      "Total words: 43479\n",
      "\n",
      ".French.\n",
      "Vocab size: 9362\n",
      "#Covered: 4765 (95.30%)\n",
      "#Missing: 235 (4.70%)\n",
      "Avg. words: 9.4114\n",
      "Total words: 47057\n"
     ]
    }
   ],
   "source": [
    "print(\"=Working set=\")\n",
    "s = working_set\n",
    "\n",
    "print(\"Size\", len(s))\n",
    "\n",
    "print(\"\\n.English.\")\n",
    "vocab_en = set()\n",
    "_ = s.apply(make_vocab, args=(vocab_en,), axis=1)\n",
    "missing_en = analyze_vocab(vocab_en, opus_vocab_en, 5000)\n",
    "\n",
    "word_counts = s.apply(lambda x:len(x[\"TEXT1\"].split()), axis=1)\n",
    "print(\"Avg. words:\", word_counts.mean())\n",
    "print(\"Total words:\", word_counts.sum())\n",
    "\n",
    "print(\"\\n.French.\")\n",
    "vocab_fr = set()\n",
    "_ = s.apply(make_vocab, args=(vocab_fr,\"2\",), axis=1)\n",
    "missing_fr = analyze_vocab(vocab_fr, opus_vocab_fr, 5000)\n",
    "\n",
    "word_counts = s.apply(lambda x:len(x[\"TEXT2\"].split()), axis=1)\n",
    "print(\"Avg. words:\", word_counts.mean())\n",
    "print(\"Total words:\", word_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked set: 5000\n"
     ]
    }
   ],
   "source": [
    "#Save to files\n",
    "\n",
    "#Add working set to picked set\n",
    "picked_set = pd.concat([picked_set, working_set], axis=0)\n",
    "picked_set = picked_set[~picked_set.index.duplicated()]\n",
    "print(\"Picked set: %i\"%(len(picked_set)))\n",
    "\n",
    "#save set\n",
    "working_set.to_csv(\"minikit.tsv\",index=False, sep=\"\\t\", header=False, quoting=csv.QUOTE_NONE, columns = ['ID1','TEXT1', 'ID2','TEXT2'])\n",
    "working_set.to_csv(\"minikit_en.csv\",index=False, sep=\"\\t\", header=False, quoting=csv.QUOTE_NONE, columns = ['TEXT1'])\n",
    "working_set.to_csv(\"minikit_fr.csv\",index=False, sep=\"\\t\", header=False, quoting=csv.QUOTE_NONE, columns = ['TEXT2'])\n",
    "working_set.to_excel(\"minikit.xlsx\", index=False, sheet_name=\"Sheet1\", columns = ['ID1','TEXT1', 'ID2','TEXT2'])\n",
    "working_set.to_pickle(\"minikit.pickle\")\n",
    "\n",
    "#save updated picked sentence ids\n",
    "#Write to file\n",
    "picked_set.to_csv('picked.tsv', sep=\"\\t\", columns = ['ID1','TEXT1', 'ID2','TEXT2'])\n",
    "picked_set.to_pickle('picked.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "working_set[\"NOWORDS2\"].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get revised translations from table\n",
    "Pre - Load working set (kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupdrop = working_set.drop_duplicates(subset=['ID1']).drop_duplicates(subset=['ID2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sentence revisions from file\n",
    "revision_table = pd.read_excel(\"minikit_EN-FR_v2alpha_Revisé-CT-ATT.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise(row):\n",
    "    try:\n",
    "        in_working_set = row[\"TEXT2\"]\n",
    "        in_revised_set = revision_table[revision_table[\"ID2\"] == row[\"ID2\"]][\"TEXT2\"].head(1)\n",
    "        if len(in_revised_set) and not in_working_set == in_revised_set.item():\n",
    "            print(\"<<<\", in_working_set)\n",
    "            print(\">>>\", in_revised_set.item())\n",
    "            print()\n",
    "            row[\"TEXT2\"] = in_revised_set.item()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(row)\n",
    "    return row\n",
    "\n",
    "working_set = working_set.apply(revise, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = working_set.apply(lambda x:len(x[\"TEXT1\"].split()), axis=1)\n",
    "word_counts.mean()\n",
    "word_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43479"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.mean()\n",
    "word_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
