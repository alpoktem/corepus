{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import nltk.data\n",
    "from progressbar import ProgressBar\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get vocabulary of size N from OPUS 50k frequency list\n",
    "\n",
    "def limit_opus_vocabulary(word_freq_file, size):\n",
    "    with open(word_freq_file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    words = [x.strip().split(' ')[0] for x in content]\n",
    "    word_freqs = [float(x.strip().split(' ')[1]) for x in content]\n",
    "    \n",
    "    vocab = []\n",
    "    freqs = []\n",
    "    for w, f in zip(words[0:size], word_freqs[0:size]):\n",
    "        vocab.append(w)\n",
    "        freqs.append(f)\n",
    "        \n",
    "    #normalize frequencies\n",
    "    sum_freqs = sum(freqs)\n",
    "    normalized_freqs = [f/sum_freqs for f in freqs]\n",
    "    \n",
    "    return vocab, normalized_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vocabulary of top N frequent words\n",
    "REFERENCE_VOCAB_SIZE = 5000\n",
    "opus_reduced_vocab, opus_reduced_freqs = limit_opus_vocabulary('src/opus_fr_50k.txt', REFERENCE_VOCAB_SIZE)\n",
    "opus_reduced_vocab_dict = {w:opus_reduced_freqs[i] for i, w in enumerate(opus_reduced_vocab)}\n",
    "\n",
    "#frequency plot on the opus vocabulary\n",
    "ax = sns.distplot(np.asarray(opus_reduced_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba filtering\n",
    "with open('etc/boynames_FR.txt', 'r') as f:\n",
    "    boynames = f.readlines()\n",
    "    boynames = [name[0:-1] for name in boynames]\n",
    "with open('etc/girlnames_FR.txt', 'r') as f:\n",
    "    girlnames = f.readlines()\n",
    "    girlnames = [name[0:-1] for name in girlnames]\n",
    "with open('etc/avoidlist_FR.txt', 'r') as f:\n",
    "    avoidlist = f.readlines()\n",
    "    avoidlist = [word[0:-1] for word in avoidlist]\n",
    "    \n",
    "MAX_COMMAS = 7\n",
    "MAX_PERIOD = 3\n",
    "MAX_QUOTE = 1\n",
    "AVOID_CHAR = ['/', '*', '<','₂','™','√','⊂','□','→','₫','=','>','[',']','^','_','~', '§', '´', '‽']\n",
    "    \n",
    "def filter_tatoeba(sent_ids, sents, max_tokens, tom_replacement_set = ['Tom'], mary_replacement_set = ['Mary'], avoid_set = []):\n",
    "    filtered_ids = []\n",
    "    filtered_sents = []\n",
    "    filtered_sent_tokens = []\n",
    "    \n",
    "    pbar = ProgressBar()\n",
    "    for index in pbar(range(len(sent_ids))):\n",
    "        sent_id = sent_ids[index]\n",
    "        sent = sents[index]\n",
    "        sent_tokens_alpha = tokenize_en(sent, True, True)\n",
    "        sent_tokens = tokenize_en(sent, True, False)\n",
    "        if len(sent_tokens_alpha) <= max_tokens and sent_tokens.count('\"') <= MAX_QUOTE and sent_tokens.count('«') <= MAX_QUOTE and sent_tokens.count(',') <= MAX_COMMAS and sent_tokens.count('.') <= MAX_PERIOD and len(intersection(avoidlist,sent_tokens)) == 0 and not any([c in sent for c in AVOID_CHAR]):\n",
    "            \n",
    "            clean_sent = sent\n",
    "            #Name replacement - tatoeba corpus has an intense use of these two names\n",
    "            if 'tom' in sent_tokens_alpha:\n",
    "                clean_sent = clean_sent.replace('Tom', tom_replacement_set[random.randint(0,len(tom_replacement_set) - 1)])\n",
    "            if 'mary' in sent_tokens_alpha:\n",
    "                clean_sent = clean_sent.replace('Mary', mary_replacement_set[random.randint(0,len(mary_replacement_set) - 1)])\n",
    "                \n",
    "            filtered_sents.append(clean_sent)\n",
    "            filtered_ids.append(int(sent_id))\n",
    "            filtered_sent_tokens.append(sent_tokens_alpha)\n",
    "            \n",
    "    return filtered_sents, filtered_sent_tokens, filtered_ids\n",
    "\n",
    "def filter_selected_sents(sents, ids, skip_sent_ids):\n",
    "    filtered_sents = []\n",
    "    filtered_ids = []\n",
    "    for i, s in zip(ids, sents):\n",
    "        if i not in skip_sent_ids:\n",
    "            filtered_sents.append(clean_sent)\n",
    "            filtered_ids.append(sent_id)\n",
    "    return filtered_sents, filtered_ids\n",
    "    \n",
    "#Tokenization and vocab building tools\n",
    "tokenizer_en = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "def tokenize_en(string, to_lower = False, clear_nonalpha = False):\n",
    "    tokens = []\n",
    "    if to_lower:\n",
    "        string = string.lower()\n",
    "    string = string.strip()\n",
    "\n",
    "    for sent in tokenizer_en.tokenize(string):\n",
    "        if clear_nonalpha:\n",
    "            sent_tokens = [tok for tok in toktok.tokenize(sent) if tok.isalpha()]\n",
    "        else:\n",
    "            sent_tokens = [tok for tok in toktok.tokenize(sent)]\n",
    "        \n",
    "        tokens.extend(sent_tokens)\n",
    "    return tokens\n",
    "\n",
    "def get_vocabulary_from_corpus(sentence_tokens):\n",
    "    vocab_dict = {}\n",
    "    total_words = 0\n",
    "    for tokens in sentence_tokens:\n",
    "        word_tokens = [tok for tok in tokens if tok.isalpha()]\n",
    "        for word_token in word_tokens:\n",
    "            if word_token in vocab_dict:\n",
    "                vocab_dict[word_token] += 1\n",
    "            else:\n",
    "                vocab_dict[word_token] = 1\n",
    "            total_words += 1\n",
    "                \n",
    "    sorted_vocab, sorted_freqs = sort_lists_wrt_list(list(vocab_dict.keys()), list(vocab_dict.values()))\n",
    "\n",
    "    #normalize frequencies \n",
    "    #sum_freqs = sum(sorted_freqs)\n",
    "    normalized_freqs = [float(f)/float(total_words) for f in sorted_freqs]\n",
    "    \n",
    "    print(\"Total #words:\", total_words)\n",
    "    \n",
    "    return sorted_vocab, normalized_freqs, total_words\n",
    "\n",
    "def sort_lists_wrt_list(mylist, values, ascending=False):\n",
    "    mylist_array = np.array(mylist)\n",
    "    values_array = np.array(values)  \n",
    "    \n",
    "    if ascending:\n",
    "        inds = values_array.argsort()[::]\n",
    "    else:\n",
    "        inds = values_array.argsort()[::-1]\n",
    "    \n",
    "    mylist_sorted = mylist_array[inds]\n",
    "    values_sorted = values_array[inds]\n",
    "    \n",
    "    return list(mylist_sorted), list(values_sorted)\n",
    "\n",
    "def sort_multilists_wrt_list(mylists, values, ascending=False):\n",
    "    mylists_arrays = [np.array(mylist) for mylist in mylists]\n",
    "    values_array = np.array(values)  \n",
    "    \n",
    "    if ascending:\n",
    "        inds = values_array.argsort()[::]\n",
    "    else:\n",
    "        inds = values_array.argsort()[::-1]\n",
    "    \n",
    "    mylists_sorted = [mylist_array[inds] for mylist_array in mylists_arrays]\n",
    "    values_sorted = values_array[inds]\n",
    "    \n",
    "    return [list(mylist_sorted) for mylist_sorted in mylists_sorted], list(values_sorted)\n",
    "\n",
    "def shuffle_parallel_lists(list_of_lists):\n",
    "    list_of_arrays = [np.array(l) for l in list_of_lists]\n",
    "    inds = list(range(0, len(list_of_arrays[0])))\n",
    "    random.shuffle(inds)\n",
    "    return [list(l[inds]) for l in list_of_arrays]\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return list(set(lst3))\n",
    "\n",
    "def difference(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value not in lst2] \n",
    "    return list(set(lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read source set\n",
    "tatoeba_corpus_path = \"src/tatoeba_sentences_CC_FR.csv\"\n",
    "with open(tatoeba_corpus_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "all_tatoeba_sent_info = [[x.strip().split(\"\\t\")[0], x.strip().split(\"\\t\")[2]] for x in content]\n",
    "all_tatoeba_sent_ids = [i[0]for i in all_tatoeba_sent_info]\n",
    "all_tatoeba_sents = [i[1]for i in all_tatoeba_sent_info]\n",
    "print('all tatoeba', len(all_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the source set\n",
    "MAX_TOKENS = 40\n",
    "clean_tatoeba_sents, clean_tatoeba_sent_tokens, clean_tatoeba_ids = filter_tatoeba(all_tatoeba_sent_ids, \n",
    "                                                                                   all_tatoeba_sents, \n",
    "                                                                                   MAX_TOKENS, \n",
    "                                                                                   boynames, \n",
    "                                                                                   girlnames, \n",
    "                                                                                   avoidlist)\n",
    "\n",
    "print('clean tatoeba', len(clean_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read exclude sets from file\n",
    "#exclude_set_files = ['gamayun_fr/FR_kit5k_ids.txt', 'gamayun_fr/FR_kit10k_ids.txt', 'gamayun_fr/FR_kit15k_ids.txt']\n",
    "exclude_set_files = []\n",
    "exclude_set = []\n",
    "for file in exclude_set_files:\n",
    "    to_exclude = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        to_exclude = [int(idx[0:-1]) for idx in lines]\n",
    "    exclude_set.extend(to_exclude)\n",
    "\n",
    "#Sort it\n",
    "exclude_set, _ = sort_lists_wrt_list(exclude_set, exclude_set, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Set include set\n",
    "include_set_ids = ready_fr_id[3]\n",
    "include_set_sents = ready_fr_sent[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter previously stored sentences (exclude_set)\n",
    "print('all set size', len(clean_tatoeba_ids))\n",
    "print('exclude set size', len(exclude_set))\n",
    "\n",
    "filtered_tatoeba_sents = []\n",
    "filtered_tatoeba_sent_tokens = []\n",
    "filtered_tatoeba_ids = []\n",
    "\n",
    "all_pointer = 0\n",
    "exclude_pointer = 0\n",
    "\n",
    "while all_pointer < len(clean_tatoeba_ids):\n",
    "    all_id = clean_tatoeba_ids[all_pointer]\n",
    "    if exclude_pointer == len(exclude_set): #already at the end of exclude set take the rest from all set\n",
    "        filtered_tatoeba_sents.append(clean_tatoeba_sents[all_pointer])\n",
    "        filtered_tatoeba_sent_tokens.append(clean_tatoeba_sent_tokens[all_pointer])\n",
    "        filtered_tatoeba_ids.append(clean_tatoeba_ids[all_pointer])\n",
    "        \n",
    "        all_pointer += 1\n",
    "    else:\n",
    "        exclude_id = exclude_set[exclude_pointer]\n",
    "        while all_id < exclude_id:\n",
    "            filtered_tatoeba_sents.append(clean_tatoeba_sents[all_pointer])\n",
    "            filtered_tatoeba_sent_tokens.append(clean_tatoeba_sent_tokens[all_pointer])\n",
    "            filtered_tatoeba_ids.append(clean_tatoeba_ids[all_pointer])\n",
    "\n",
    "            all_pointer += 1\n",
    "            try:\n",
    "                all_id = clean_tatoeba_ids[all_pointer]\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        all_pointer += 1\n",
    "        exclude_pointer += 1\n",
    "\n",
    "print('filtered tatoeba', len(filtered_tatoeba_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 1 - Read include set as initial set\n",
    "naive_corpus_sents_nonfrequent_takenout = include_set_sents\n",
    "naive_corpus_ids_nonfrequent_takenout = include_set_ids\n",
    "naive_corpus_senttokens_nonfrequent_takenout = [tokenize_en(sent, True, True) for sent in include_set_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 2 - Naive corpus selection: Select C random sentences\n",
    "shuffled_sents, shuffled_sent_tokens, shuffled_ids = shuffle_parallel_lists([filtered_tatoeba_sents, filtered_tatoeba_sent_tokens, filtered_tatoeba_ids])\n",
    "\n",
    "naive_corpus_sents = shuffled_sents[0:CORPUS_SIZE]\n",
    "naive_corpus_sent_tokens = shuffled_sent_tokens[0:CORPUS_SIZE]\n",
    "naive_corpus_ids = shuffled_ids[0:CORPUS_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 2 - Build vocabulary on the naive corpus\n",
    "corpus = naive_corpus_sents\n",
    "corpus_tokenized = naive_corpus_sent_tokens\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "corpus_vocab, corpus_freqs, corpus_total_words = get_vocabulary_from_corpus(corpus_tokenized)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, corpus_vocab)\n",
    "missing_vocab = difference(reference_vocab, corpus_vocab)\n",
    "extra_vocab = difference(corpus_vocab, reference_vocab)\n",
    "still_missing_vocab = missing_vocab\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"missing\", len(missing_vocab))\n",
    "print(\"extra\", len(extra_vocab))\n",
    "print(\"%.2f%% missing\"%(len(missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 2 - Print out missing words in the order that they appear in opus vocabulary\n",
    "missing_indexes = [opus_vocab.index(w) for w in still_missing_vocab]\n",
    "sorted_missing_vocab, sorted_missing_index = sort_lists_wrt_list(missing_vocab, missing_indexes, True)\n",
    "\n",
    "for w, index in zip(sorted_missing_vocab, sorted_missing_index):\n",
    "    print(\"%s - opus ind:%i\"%(w, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 2 - Take out sentences in the corpus: those with non-frequent words and randomly\n",
    "RANDOM_SENTENCE_TAKEOUT = 5 #size of die to roll, put -1 to skip\n",
    "count = 0\n",
    "naive_corpus_sents_nonfrequent_takenout = []\n",
    "naive_corpus_ids_nonfrequent_takenout = []\n",
    "naive_corpus_senttokens_nonfrequent_takenout = []\n",
    "for sent, sent_tokens, sent_id in zip(naive_corpus_sents, naive_corpus_sent_tokens, naive_corpus_ids):\n",
    "    nonfrequent = False\n",
    "    for token in sent_tokens:\n",
    "        if token not in opus_reduced_vocab: #condition for non-frequent = not in opus_reduced_vocab\n",
    "            nonfrequent = True\n",
    "            break\n",
    "            \n",
    "    #Random sentence extraction\n",
    "    if not RANDOM_SENTENCE_TAKEOUT == -1:\n",
    "        dice = random.randint(0,RANDOM_SENTENCE_TAKEOUT)\n",
    "    else:\n",
    "        dice = 1\n",
    "\n",
    "    if not dice == 0:        \n",
    "        if nonfrequent:\n",
    "            count += 1\n",
    "        else:\n",
    "            naive_corpus_sents_nonfrequent_takenout.append(sent)\n",
    "            naive_corpus_ids_nonfrequent_takenout.append(sent_id)\n",
    "            naive_corpus_senttokens_nonfrequent_takenout.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 3 - Start with an empty corpus\n",
    "naive_corpus_sents_nonfrequent_takenout = []\n",
    "naive_corpus_senttokens_nonfrequent_takenout = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary on the corpus\n",
    "corpus = naive_corpus_sents_nonfrequent_takenout\n",
    "corpus_tokenized = naive_corpus_senttokens_nonfrequent_takenout\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "reduced_corpus_vocab, reduced_corpus_freqs, reduced_corpus_total_words = get_vocabulary_from_corpus(corpus_tokenized)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, reduced_corpus_vocab)\n",
    "missing_vocab = difference(reference_vocab, reduced_corpus_vocab)\n",
    "extra_vocab = difference(reduced_corpus_vocab, reference_vocab)\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(reduced_corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"missing\", len(missing_vocab))\n",
    "print(\"extra\", len(extra_vocab))\n",
    "print(\"%f\"%(len(missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Search in rest of the sentences in tatoeba which has the missing words\n",
    "missing_vocab_replacement_coefs = [(REFERENCE_VOCAB_SIZE - opus_reduced_vocab.index(token))/1000 if token in missing_vocab else 0.0 for token in opus_reduced_vocab]\n",
    "original_missing_vocab_replacement_coefs = missing_vocab_replacement_coefs.copy()\n",
    "\n",
    "count = 0 \n",
    "no_missing_in_candidate = []\n",
    "candidate_replacement_score = []\n",
    "candidate_sent_index = []\n",
    "candidate_missing_word_indexes = []\n",
    "candidate_missing_word_coefs = []\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(filtered_tatoeba_sents))):\n",
    "    if i < CORPUS_SIZE:\n",
    "        continue\n",
    "        \n",
    "    sent = filtered_tatoeba_sents[i]    \n",
    "    sent_tokens = filtered_tatoeba_sent_tokens[i]\n",
    "    missing_in_sent = intersection(sent_tokens, missing_vocab)\n",
    "    no_missing = len(missing_in_sent)\n",
    "    extra_in_sent = difference(sent_tokens, opus_reduced_vocab)\n",
    "    no_extra = len(extra_in_sent)\n",
    "\n",
    "    \n",
    "    missing_word_indexes = [opus_reduced_vocab.index(token) for token in missing_in_sent]\n",
    "    missing_word_replacement_coefs = [missing_vocab_replacement_coefs[token_index] for token_index in missing_word_indexes]    \n",
    "    replacement_score = sum(missing_word_replacement_coefs)\n",
    "    \n",
    "    \n",
    "    #if no_missing > 0 and no_extra == 0 :\n",
    "    no_missing_in_candidate.append(no_missing)\n",
    "    candidate_replacement_score.append(replacement_score)\n",
    "    candidate_sent_index.append(i)\n",
    "    candidate_missing_word_indexes.append(missing_word_indexes)\n",
    "    candidate_missing_word_coefs.append(missing_word_replacement_coefs)\n",
    "    count += 1\n",
    "    \n",
    "[candidate_sent_index_sorted, candidate_missing_word_coefs_sorted, candidate_missing_word_indexes_sorted], candidate_replacement_score_sorted = sort_multilists_wrt_list([candidate_sent_index, candidate_missing_word_coefs, candidate_missing_word_indexes], candidate_replacement_score)    \n",
    "        \n",
    "print(\"no candidate fill-in sents\", len(candidate_sent_index_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To restore original coefficients before rescoring\n",
    "missing_vocab_replacement_coefs = original_missing_vocab_replacement_coefs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescore candidates. Replacement coefficient of a missing word is reduced by once it is seen. \n",
    "new_candidate_replacement_score = []\n",
    "new_candidate_missing_word_coefs = []\n",
    "for candidate_score, candidate_sent, covered_indexes in zip(candidate_replacement_score_sorted, candidate_sent_index_sorted, candidate_missing_word_indexes_sorted):\n",
    "\n",
    "    new_missing_word_replacement_coefs = [missing_vocab_replacement_coefs[token_index] for token_index in covered_indexes]\n",
    "    new_replacement_score = sum(new_missing_word_replacement_coefs)\n",
    "    new_candidate_missing_word_coefs.append(new_missing_word_replacement_coefs)\n",
    "    new_candidate_replacement_score.append(new_replacement_score)\n",
    "    \n",
    "    #Reduce coef of less frequent words more than frequent words in order to maintain frequency\n",
    "    for token_index in covered_indexes:\n",
    "        if token_index < 1000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*60/100\n",
    "        elif token_index < 2000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*70/100\n",
    "        elif token_index < 2000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*80/100\n",
    "        elif token_index < 3000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*90/100\n",
    "        elif token_index < 4000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "        elif token_index < 5000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "        elif token_index < 6000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "        elif token_index < 7000:\n",
    "            missing_vocab_replacement_coefs[token_index] -= missing_vocab_replacement_coefs[token_index]*100/100\n",
    "\n",
    "\n",
    "[new_candidate_sent_index_sorted, new_candidate_missing_word_coefs_sorted, new_candidate_missing_word_indexes_sorted], new_candidate_replacement_score_sorted = sort_multilists_wrt_list([candidate_sent_index_sorted, candidate_missing_word_coefs_sorted, candidate_missing_word_indexes_sorted], new_candidate_replacement_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) print replacement candidate sentences sorted with their score sum\n",
    "\n",
    "# sentence_index = candidate_sent_index_sorted\n",
    "# sentence_coefs = candidate_missing_word_coefs_sorted\n",
    "# sentence_missing_words = candidate_missing_word_indexes_sorted\n",
    "# sentence_score = candidate_replacement_score_sorted\n",
    "\n",
    "sentence_index = new_candidate_sent_index_sorted\n",
    "sentence_coefs = new_candidate_missing_word_coefs_sorted\n",
    "sentence_missing_words = new_candidate_missing_word_indexes_sorted\n",
    "sentence_score = new_candidate_replacement_score_sorted\n",
    "\n",
    "count = 0\n",
    "no_fill_in = CORPUS_SIZE - len(naive_corpus_sents_nonfrequent_takenout)\n",
    "for s_index, s_score, missing_words, coefs in zip(sentence_index, sentence_score, sentence_missing_words, sentence_coefs):\n",
    "    missing_info=\"\"\n",
    "    for token_index, coef in zip(missing_words, coefs):\n",
    "        missing_info += opus_reduced_vocab[token_index]\n",
    "        missing_info += \"(\"\n",
    "        missing_info += str(token_index)\n",
    "        missing_info += \"/\"\n",
    "        missing_info += str(coef)\n",
    "        missing_info += \") \"\n",
    "    \n",
    "    if no_fill_in - count <= 0:\n",
    "        print(\"=========================================================\")\n",
    "    \n",
    "    print(\"%s -- %f %s\"%(filtered_tatoeba_sents[s_index], s_score, missing_info))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Check duplication in fill-in set\n",
    "fillin_from = new_candidate_sent_index_sorted\n",
    "fillin_from_tokens = [filtered_tatoeba_sent_tokens[index] for index in new_candidate_sent_index_sorted]\n",
    "\n",
    "cover_dict = {missing_word:0 for missing_word in missing_vocab}\n",
    "for sent_tokens in fillin_from_tokens:\n",
    "    for tok in sent_tokens:\n",
    "        if tok in cover_dict.keys():\n",
    "            cover_dict[tok] += 1\n",
    "            if cover_dict[tok] > 1:\n",
    "                print(\"%s is covered %i times\"%(tok, cover_dict[tok]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in candidate sentences to the reduced corpus\n",
    "\n",
    "fillin_from = new_candidate_sent_index_sorted\n",
    "fillin_from_tokens = [filtered_tatoeba_sent_tokens[index] for index in new_candidate_sent_index_sorted]\n",
    "\n",
    "naive_corpus_sents_frequent_filledin = naive_corpus_sents_nonfrequent_takenout.copy()\n",
    "naive_corpus_senttokens_frequent_filledin = naive_corpus_senttokens_nonfrequent_takenout.copy()\n",
    "naive_corpus_ids_frequent_filledin = naive_corpus_ids_nonfrequent_takenout.copy()\n",
    "print(\"reduced corpus\", len(naive_corpus_sents_nonfrequent_takenout))\n",
    "for sent_index in new_candidate_sent_index_sorted:\n",
    "    sent = filtered_tatoeba_sents[sent_index]\n",
    "    sent_id = filtered_tatoeba_ids[sent_index]\n",
    "    sent_tokens = filtered_tatoeba_sent_tokens[sent_index]\n",
    "    naive_corpus_sents_frequent_filledin.append(sent)\n",
    "    naive_corpus_senttokens_frequent_filledin.append(sent_tokens)\n",
    "    naive_corpus_ids_frequent_filledin.append(sent_id)\n",
    "    if len(naive_corpus_sents_frequent_filledin) >= CORPUS_SIZE:\n",
    "        break\n",
    "print(\"filled-in corpus\", len(naive_corpus_sents_frequent_filledin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filled-in corpus stats\n",
    "corpus = naive_corpus_sents_frequent_filledin\n",
    "corpus_tokens = naive_corpus_senttokens_frequent_filledin\n",
    "reference_vocab = opus_reduced_vocab\n",
    "\n",
    "modified_corpus_vocab, modified_corpus_freqs, modified_corpus_total_words = get_vocabulary_from_corpus(corpus_tokens)\n",
    "\n",
    "intersecting_vocab = intersection(reference_vocab, modified_corpus_vocab)\n",
    "still_missing_vocab = difference(reference_vocab, modified_corpus_vocab)\n",
    "still_extra_vocab = difference(modified_corpus_vocab, reference_vocab)\n",
    "\n",
    "print(\"corpus size\", len(corpus))\n",
    "print(\"corpus_vocab\", len(modified_corpus_vocab))\n",
    "print(\"reference vocab\", len(reference_vocab))\n",
    "print(\"intersecting\", len(intersecting_vocab))\n",
    "print(\"still missing\", len(still_missing_vocab))\n",
    "print(\"still extra\", len(still_extra_vocab))\n",
    "print(\"%f\"%(len(still_missing_vocab)/len(reference_vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write corpus to file (run after doing counts)\n",
    "\n",
    "CORPUS_DIR = 'gamayun_fr-v2'\n",
    "CORPUS_ID = 'FR_kit5k-v2'\n",
    "!mkdir -p $CORPUS_DIR\n",
    "\n",
    "corpus_path = os.path.join(CORPUS_DIR, CORPUS_ID)\n",
    "corpus_sents_to_write = naive_corpus_sents_frequent_filledin\n",
    "corpus_ids_to_write = naive_corpus_ids_frequent_filledin\n",
    "\n",
    "with open(corpus_path +'_sentences.txt', 'w') as f:\n",
    "    for s in corpus_sents_to_write:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(corpus_path +'_ids.txt', 'w') as f:\n",
    "    for s in corpus_ids_to_write:\n",
    "        #print(type(s))\n",
    "        f.write(str(s) + \"\\n\")\n",
    "    \n",
    "with open(corpus_path +'_vocab.txt', 'w') as f:\n",
    "    for s in modified_corpus_vocab:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(corpus_path +'_missingvocab.txt', 'w') as f:\n",
    "    for s in still_missing_vocab:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "# with open(corpus_path +'_linkeduntil.txt', 'w') as f:\n",
    "#     f.write(str(len(include_set_ids)) + \"\\n\")        \n",
    "#-----FIN-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting already parallel sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "response = urllib3.urlopen(\"https://tatoeba.org/eng/sentences/show/3009208\")\n",
    "page_source = response.read()\n",
    "\n",
    "with open('3009208.html', 'wb') as f:\n",
    "    f.write(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "try: \n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "    from bs4 import BeautifulSoup\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get English word ids\n",
    "sent_id_files = ['/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/etc/EN_kit5k-v1_ids.txt']#,\n",
    "#                  '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/etc/EN_kit10k-v1_ids.txt',\n",
    "#                  '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/etc/EN_kit15k-v1_ids.txt',\n",
    "#                  '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/etc/EN_kit30k-v1_ids.txt']\n",
    "\n",
    "sent_files = ['/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/kits/EN_kit5k-v1_sentences.txt']#,\n",
    "#               '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/kits/EN_kit10k-v1_sentences.txt',\n",
    "#               '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/kits/EN_kit15k-v1_sentences.txt',\n",
    "#               '/Users/alp/Documents/TWB/data/Gamayun-kits/core/v1/en/kits/EN_kit30k-v1_sentences.txt']\n",
    "\n",
    "#sent_id_files = ['test_eng_sents.txt']\n",
    "\n",
    "english_sent_ids = []\n",
    "english_sent_setid = []\n",
    "english_dict = {}\n",
    "\n",
    "for i, (file_id, file_sent) in enumerate(zip(sent_id_files, sent_files)):\n",
    "    with open(file_id, 'r') as f_id, open(file_sent, 'r') as f_s:\n",
    "        for line_id, line_sent in zip(f_id, f_s):\n",
    "            sent_id = line_id.strip()\n",
    "            sent = line_sent.strip()\n",
    "            english_sent_ids.append(sent_id)\n",
    "            english_sent_setid.append(i)\n",
    "            english_dict[sent_id] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dict['3009208']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine which ones have french translation\n",
    "\n",
    "french_sent_ids = []\n",
    "french_sent_texts = []\n",
    "\n",
    "pbar = ProgressBar()\n",
    "for en_sent in pbar(english_sent_ids):\n",
    "    try:\n",
    "        link = \"https://tatoeba.org/eng/sentences/show/\" + en_sent\n",
    "        print(link)\n",
    "        f = urlopen(link)\n",
    "        myhtml = f.read()\n",
    "\n",
    "        #default values\n",
    "        french_sent_text = \"\"\n",
    "        french_sent_id = \"-1\"\n",
    "\n",
    "        #Parse and find if there is French translation\n",
    "        parsed_html = BeautifulSoup(myhtml)\n",
    "        trans_divs = parsed_html.body.find_all('div', attrs={'class':'translation'})\n",
    "        for trans_div in trans_divs:\n",
    "            lang_div = trans_div.find('div', attrs={'class':'lang'})\n",
    "            if lang_div.find('img').attrs['title'] == 'French':\n",
    "                french_sent_text = trans_div.find('div', attrs={'class':'text'}).text.strip()\n",
    "                french_sent_id = trans_div.find_all('md-button', attrs={'class': 'md-icon-button'})[-1].attrs['href'].split(\"/\")[-1]\n",
    "                break\n",
    "\n",
    "        french_sent_texts.append(french_sent_text)\n",
    "        french_sent_ids.append(french_sent_id)\n",
    "    except Exception as e:\n",
    "        print(\"A problem occured. All went well until %i\"%len(french_sent_ids))\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('html.html', 'wb') as f:\n",
    "    f.write(myhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://tatoeba.org/eng/sentences/show/3009208\"\n",
    "f = urlopen(link)\n",
    "myhtml = f.read()\n",
    "parsed_html = BeautifulSoup(myhtml)\n",
    "trans_divs = parsed_html.body.find_all('div', attrs={'class':'translation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_div = trans_divs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_div = trans_div.find('div', attrs={'class':'lang'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_div.find('img').attrs['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print(len(english_sent_ids))\n",
    "print(len(french_sent_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('french_ids_backup.txt', 'w') as f:\n",
    "#     for i in french_sent_ids:\n",
    "#         f.write(i + \"\\n\")\n",
    "        \n",
    "with open('french_sents_backup.txt', 'w') as f:\n",
    "    for i in french_sent_texts:\n",
    "        f.write(i + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_en_id = {0:[], 1:[], 2:[], 3:[]}\n",
    "ready_en_sent = {0:[], 1:[], 2:[], 3:[]}\n",
    "ready_fr_id = {0:[], 1:[], 2:[], 3:[]}\n",
    "ready_fr_sent = {0:[], 1:[], 2:[], 3:[]}\n",
    "\n",
    "count = 0\n",
    "for set_id, en_id, fr_id, fr_sent in zip(english_sent_setid, english_sent_ids, french_sent_ids, french_sent_texts):\n",
    "    if not fr_id == '-1':\n",
    "        count += 1\n",
    "        print(\"EN: %s (%i), FR: |%s|\"%(en_id, set_id, fr_id))\n",
    "        ready_en_id[set_id].append(en_id)\n",
    "        ready_en_sent[set_id].append(english_dict[en_id])\n",
    "        ready_fr_id[set_id].append(fr_id)\n",
    "        ready_fr_sent[set_id].append(fr_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ready_fr_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Où ces photos ont-elles été prises ?\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "DRIVER_BIN = \"/Users/alp/Downloads/chromedriver\"\n",
    "\n",
    "# Setup driver\n",
    "driver = webdriver.Chrome(DRIVER_BIN)\n",
    "driver.get('https://tatoeba.org/fra/sentences/show/3009208')\n",
    "\n",
    "# Find fra translation\n",
    "e = driver.find_element_by_xpath('//div[@class=\"translation\"]/div/div/div[@lang=\"fr\"]/span')\n",
    "\n",
    "# Show\n",
    "print(e.text)\n",
    "# 'Où ces photos ont-elles été prises ?'\n",
    "\n",
    "# Don't forget to close the browser when done!\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
